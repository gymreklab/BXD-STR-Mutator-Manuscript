{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare supplementary data with mutation summary info\n",
    "\n",
    "This notebook generates Supplementary Datasets 1-3, plus the `auto_info` dataframe used by future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melissagymrek/opt/miniconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trtools.utils.utils\n",
    "\n",
    "from mutation_pattern_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load mutation datasets ####\n",
    "DATADIR = '../../BXD-STR-Mutator-Manuscript/outs'\n",
    "auto = pd.read_csv(os.path.join(DATADIR, 'denovo_strs_filtered.csv'))\n",
    "\n",
    "#### Load other metadata ###\n",
    "motif_info = pd.read_csv(os.path.join(DATADIR, 'motif_info.csv'))\n",
    "calls_info = pd.read_csv(os.path.join(DATADIR, 'all_repcn_proc_nosegdup_nolowcr_segreg.csv'))\n",
    "chr13_gt = pd.read_csv(os.path.join(DATADIR, 'fou_gt_at_peak_chr13.tsv'), sep='\\t')\n",
    "strains_info = pd.read_csv(os.path.join(DATADIR, 'strain_info.csv'))\n",
    "founder_labels = pd.read_csv(os.path.join(DATADIR, 'all_foulab_nosegdup_nolowcr_noalshared_padded_imp.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare locus-level summary of mutation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Summarize missingness at each locus ####\n",
    "\n",
    "# Note: missingness info needed to compute mutation rates, since we \n",
    "# need to know how many actual calls there were\n",
    "\n",
    "# First, summarize missingness based on if the strain has sa B or D haplotype at the chr13 QTL\n",
    "calls_info_missing = calls_info.iloc[:, 0:3]\n",
    "calls_info_D = calls_info[list(chr13_gt[chr13_gt['fou_gt']=='D']['strain'])]\n",
    "calls_info_missing.insert(3, 'missing_D', calls_info_D.isnull().sum(axis=1))\n",
    "calls_info_missing['calls_D'] = calls_info_missing['missing_D'].apply(lambda x: len(calls_info_D.columns) - x )\n",
    "\n",
    "calls_info_B = calls_info[list(chr13_gt[chr13_gt['fou_gt']=='B']['strain'])]\n",
    "calls_info_missing.insert(5, 'missing_B', calls_info_B.isnull().sum(axis=1))\n",
    "calls_info_missing['calls_B'] = calls_info_missing['missing_B'].apply(lambda x: len(calls_info_B.columns) - x )\n",
    "\n",
    "# Now, summarize based on the founder haplotype on which the mutation falls\n",
    "calls_info['chr_pos'] = calls_info.apply(lambda x: f'{x.chr}_{x.pos}', axis=1)\n",
    "founder_labels['chr_pos'] = founder_labels.apply(lambda x: f'{x.chr}_{x.pos}', axis=1)\n",
    "calls_info = calls_info[calls_info.chr_pos.isin(founder_labels.chr_pos)]\n",
    "founder_labels = founder_labels[founder_labels.chr_pos.isin(calls_info.chr_pos)]\n",
    "calls_info = calls_info.sort_values(['chr', 'pos'])\n",
    "founder_labels = founder_labels.sort_values(['chr', 'pos'])\n",
    "\n",
    "calls_BB = []\n",
    "missing_BB = []\n",
    "calls_DD = []\n",
    "missing_DD = []\n",
    "calls_BD = []\n",
    "missing_BD = []\n",
    "calls_DB = []\n",
    "missing_DB = []\n",
    "\n",
    "D_chr13 = list(chr13_gt[chr13_gt['fou_gt']=='D']['strain'])\n",
    "B_chr13 = list(chr13_gt[chr13_gt['fou_gt']=='B']['strain'])\n",
    "\n",
    "for index, row in calls_info.iterrows():\n",
    "    B_col = founder_labels.columns[[x=='B' for x in founder_labels.iloc[index]]]\n",
    "    D_col = founder_labels.columns[[x=='D' for x in founder_labels.iloc[index]]]\n",
    "    BB_col = list(set(B_col) & set(B_chr13))\n",
    "    DD_col = list(set(D_col) & set(D_chr13))\n",
    "    BD_col = list(set(B_col) & set(D_chr13))\n",
    "    DB_col = list(set(D_col) & set(B_chr13))\n",
    "\n",
    "    calls_BB.append(row[BB_col].notnull().sum())\n",
    "    missing_BB.append(row[BB_col].isnull().sum())\n",
    "    calls_DD.append(row[DD_col].notnull().sum())\n",
    "    missing_DD.append(row[DD_col].isnull().sum())\n",
    "    calls_BD.append(row[BD_col].notnull().sum())\n",
    "    missing_BD.append(row[BD_col].isnull().sum())\n",
    "    calls_DB.append(row[DB_col].notnull().sum())\n",
    "    missing_DB.append(row[DB_col].isnull().sum())\n",
    "calls_info_missing.insert(7,'calls_BB', calls_BB)\n",
    "calls_info_missing.insert(8,'missing_BB', missing_BB)\n",
    "calls_info_missing.insert(9,'calls_DD', calls_DD)\n",
    "calls_info_missing.insert(10,'missing_DD', missing_DD)\n",
    "calls_info_missing.insert(11,'calls_BD', calls_BD)\n",
    "calls_info_missing.insert(12,'missing_BD', missing_BD)\n",
    "calls_info_missing.insert(13,'calls_DB', calls_DB)\n",
    "calls_info_missing.insert(14,'missing_DB', missing_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Attach motif info and other metadata to autosome dataframe ####\n",
    "\n",
    "# Number of mutations per locus\n",
    "num_mut = auto.groupby([\"chr\", \"pos\"]).size().to_frame('num_mut')\n",
    "auto_info = pd.merge(auto, num_mut, on=['chr', 'pos'])\n",
    "\n",
    "# Add motif information\n",
    "auto_info = pd.merge(auto_info, motif_info[[\"chr\",\"pos\",\"end\",\"motif\",\"motif_len\"]], on=[\"chr\",\"pos\",\"end\"])\n",
    "auto_info[\"motif\"] = auto_info[\"motif\"].apply(trtools.utils.utils.GetCanonicalMotif)\n",
    "\n",
    "# Add founder at chr13 info for each strain\n",
    "auto_info = pd.merge(auto_info, chr13_gt[['strain', 'fou_gt']], on=['strain'])\n",
    "auto_info.rename(columns = {'fou_gt_y':'fou_gt_chr13'}, inplace = True)\n",
    "\n",
    "# Summarize missingness\n",
    "auto_info = pd.merge(auto_info, calls_info_missing, on=[\"chr\",\"pos\",\"end\"])\n",
    "\n",
    "# Add founder calls\n",
    "calls_founder = calls_info[['chr', 'pos', 'end', 'DBA', 'C57BL']].copy()\n",
    "calls_founder['DBA'] = calls_founder['DBA'].apply(GetFounderCall)\n",
    "calls_founder['C57BL'] = calls_founder['C57BL'].apply(GetFounderCall)\n",
    "auto_info = pd.merge(auto_info, calls_founder, on=['chr', 'pos', 'end'])\n",
    "auto_info[\"fou_rn\"] = auto_info[\"fou_gt_x\"].apply(GetFounderCall)\n",
    "\n",
    "# Add locus-level mutation summary stats\n",
    "summary_ops = {\n",
    "    \"expan_perc\": expan_perc,\n",
    "    \"num_B\": num_B,\n",
    "    \"num_D\": num_D,\n",
    "    \"num_B_founder\": num_B_founder,\n",
    "    \"num_D_founder\": num_D_founder,\n",
    "    \"num_DD\": num_DD_founder_gt13,\n",
    "    \"num_DB\": num_DB_founder_gt13,\n",
    "    \"num_BD\": num_BD_founder_gt13,\n",
    "    \"num_BB\": num_BB_founder_gt13,\n",
    "    \"num_expan\": num_expan,\n",
    "    \"num_contr\": num_contr\n",
    "}\n",
    "grouped_mut = auto_info.groupby([\"chr\", \"pos\"])\n",
    "for key, val in summary_ops.items():\n",
    "    df = grouped_mut.apply(val).to_frame(key)\n",
    "    auto_info = pd.merge(auto_info, df, on=['chr','pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Annotate expansion rate and size info by B/D ####\n",
    "# expan_B, contr_B, expan_D, contr_D\n",
    "expan_dict = defaultdict(lambda: [0,0,0,0])\n",
    "# expan_BB, contr_BB, expan_DD, contr_DD, expan_BD, contr_BD, expan_DB, contr_DB\n",
    "expan_gt13founder_dict = defaultdict(lambda: [0,0,0,0, 0,0,0,0])\n",
    "\n",
    "expan_sizes_B = defaultdict(list)\n",
    "expan_sizes_D = defaultdict(list)\n",
    "\n",
    "for index, row in auto_info.iterrows():\n",
    "    pos = f\"{row['chr']}_{row['pos']}\"\n",
    "    if row['fou_gt_chr13'] == 'B':\n",
    "        expan_sizes_B[pos].append(row['delta_fou']*row['expand_sign'])\n",
    "        if row['expand_type'] == 'expan':\n",
    "            expan_dict[pos][0] += 1\n",
    "            #BB\n",
    "            if row['founder'] =='B':\n",
    "                expan_gt13founder_dict[pos][0] += 1\n",
    "            #DB\n",
    "            else: \n",
    "                expan_gt13founder_dict[pos][6] += 1\n",
    "        #contractions\n",
    "        else:\n",
    "            expan_dict[pos][1] += 1\n",
    "            if row['founder'] =='B':\n",
    "                expan_gt13founder_dict[pos][1] += 1\n",
    "            else: \n",
    "                expan_gt13founder_dict[pos][7] += 1\n",
    "    else:\n",
    "        expan_sizes_D[pos].append(row['delta_fou']*row['expand_sign'])\n",
    "        if row['expand_type'] == 'expan':\n",
    "            expan_dict[pos][2] += 1\n",
    "            #DD\n",
    "            if row['founder'] =='D':\n",
    "                expan_gt13founder_dict[pos][2] += 1\n",
    "            #BD\n",
    "            else: \n",
    "                expan_gt13founder_dict[pos][4] += 1\n",
    "        else:\n",
    "            expan_dict[pos][3] += 1\n",
    "            if row['founder'] =='D':\n",
    "                expan_gt13founder_dict[pos][3] += 1\n",
    "            else: \n",
    "                expan_gt13founder_dict[pos][5] += 1\n",
    "\n",
    "auto_info['expan_B'] = 0\n",
    "auto_info['contr_B'] = 0\n",
    "auto_info['expan_D'] = 0\n",
    "auto_info['contr_D'] = 0\n",
    "auto_info['expan_sizes_B'] = np.empty((len(auto_info), 0)).tolist()\n",
    "auto_info['expan_sizes_D'] = np.empty((len(auto_info), 0)).tolist()\n",
    "\n",
    "auto_info['expan_BB'] = 0\n",
    "auto_info['contr_BB'] = 0\n",
    "auto_info['expan_DD'] = 0\n",
    "auto_info['contr_DD'] = 0\n",
    "auto_info['expan_BD'] = 0\n",
    "auto_info['contr_BD'] = 0\n",
    "auto_info['expan_DB'] = 0\n",
    "auto_info['contr_DB'] = 0\n",
    "\n",
    "for index, row in auto_info.iterrows():\n",
    "    pos = f\"{row['chr']}_{row['pos']}\"\n",
    "    auto_info.at[index, 'expan_B'] = expan_dict[pos][0]\n",
    "    auto_info.at[index, 'contr_B'] = expan_dict[pos][1]\n",
    "    auto_info.at[index, 'expan_D'] = expan_dict[pos][2]\n",
    "    auto_info.at[index, 'contr_D'] = expan_dict[pos][3]\n",
    "    auto_info.at[index, 'expan_sizes_B'] = expan_sizes_B[pos]\n",
    "    auto_info.at[index, 'expan_sizes_D'] = expan_sizes_D[pos]\n",
    "    \n",
    "    auto_info.at[index, 'expan_BB'] = expan_gt13founder_dict[pos][0]\n",
    "    auto_info.at[index, 'contr_BB'] = expan_gt13founder_dict[pos][1]\n",
    "    auto_info.at[index, 'expan_DD'] = expan_gt13founder_dict[pos][2]\n",
    "    auto_info.at[index, 'contr_DD'] = expan_gt13founder_dict[pos][3]\n",
    "\n",
    "    auto_info.at[index, 'expan_BD'] = expan_gt13founder_dict[pos][4]\n",
    "    auto_info.at[index, 'contr_BD'] = expan_gt13founder_dict[pos][5]\n",
    "    auto_info.at[index, 'expan_DB'] = expan_gt13founder_dict[pos][6]\n",
    "    auto_info.at[index, 'contr_DB'] = expan_gt13founder_dict[pos][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save auto_info for use in future notebooks\n",
    "auto_info.to_csv(\"../outs/auto_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output supplementary datasets 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Supplementary Dataset 1 ####\n",
      "Number of mutations: 52812\n",
      "Number of unique strains: 151\n",
      "Number of unique loci: 18119\n"
     ]
    }
   ],
   "source": [
    "#### Supp dataset 1: All mutations ####\n",
    "shared_cols = [\"chr\",\"pos\",\"end\", \"motif\", \"motif_len\", \"DBA\", \"C57BL\"]\n",
    "supp_data1 = auto_info[shared_cols+[\"strain\",\"RN_A\",\"RN_B\",\"founder\", \\\n",
    "                                   \"fou_gt_chr13\",\"fou_rn\", \\\n",
    "                                   \"delta_fou\",\"expand_sign\",\"expand_type\"]]\n",
    "\n",
    "print(\"### Supplementary Dataset 1 ####\")\n",
    "print(\"Number of mutations: %s\"%supp_data1.shape[0])\n",
    "print(\"Number of unique strains: %s\"%len(set(supp_data1[\"strain\"])))\n",
    "print(\"Number of unique loci: %s\"%supp_data1[[\"chr\",\"pos\"]].drop_duplicates().shape[0])\n",
    "\n",
    "# Output to a file\n",
    "supp_data1.sort_values([\"chr\",\"pos\"]).to_csv(\"../pdfs/SupplementaryDataset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Supplementary Dataset 2 ####\n",
      "Number of loci: 18119\n"
     ]
    }
   ],
   "source": [
    "#### Supp dataset 2: Locus-level summary ####\n",
    "auto_info[\"mut_sizes_B\"] = auto_info[\"expan_sizes_B\"].apply(lambda x: \",\".join([str(item) for item in x]))\n",
    "auto_info[\"mut_sizes_D\"] = auto_info[\"expan_sizes_D\"].apply(lambda x: \",\".join([str(item) for item in x]))\n",
    "supp_data2 = auto_info[shared_cols + \\\n",
    "                       ['num_mut', 'num_B', 'num_D', 'num_B_founder', 'num_D_founder', \\\n",
    "                        'num_DD', 'num_DB', 'num_BD', 'num_BB', \\\n",
    "                        'num_expan', 'num_contr', 'expan_B', 'contr_B', 'expan_D', 'contr_D', \\\n",
    "                        'mut_sizes_B', 'mut_sizes_D', \\\n",
    "                        'expan_BB', 'expan_DD', 'expan_DB', 'expan_BD', \\\n",
    "                        'contr_BB', 'contr_DD', 'contr_BD', 'contr_DB', \\\n",
    "                        'missing_D', 'calls_D', 'missing_B', 'calls_B', \\\n",
    "                        'calls_BB', 'calls_DD', 'calls_BD', 'calls_DB', \\\n",
    "                        'missing_BB', 'missing_DD', 'missing_BD', 'missing_DB']].drop_duplicates()\n",
    "\n",
    "print(\"### Supplementary Dataset 2 ####\")\n",
    "print(\"Number of loci: %s\"%supp_data2.shape[0])\n",
    "\n",
    "# Output to a file\n",
    "supp_data2.sort_values([\"chr\",\"pos\"]).to_csv(\"../pdfs/SupplementaryDataset2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Supplementary Dataset 3 ####\n",
      "Number of loci: 76634\n"
     ]
    }
   ],
   "source": [
    "#### Supp dataset 3: Summary of call-missingness at all STRs analyzed ####\n",
    "supp_data3 = calls_info_missing\n",
    "\n",
    "print(\"### Supplementary Dataset 3 ####\")\n",
    "print(\"Number of loci: %s\"%supp_data3.shape[0])\n",
    "\n",
    "# Output to a file\n",
    "supp_data3.sort_values([\"chr\",\"pos\"]).to_csv(\"../pdfs/SupplementaryDataset3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
